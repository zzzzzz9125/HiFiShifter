import sys
import os

# Fix for "Unable to find the current directory"
try:
    os.getcwd()
except (FileNotFoundError, OSError):
    # Fallback to script directory
    os.chdir(os.path.dirname(os.path.abspath(__file__)))

# Fix for PyQt6 DLL load failed on Windows Conda environments
if os.name == 'nt':
    # Add Library/bin to PATH so DLLs can be found
    conda_lib_bin = os.path.join(sys.prefix, 'Library', 'bin')
    if os.path.exists(conda_lib_bin):
        os.environ['PATH'] = conda_lib_bin + os.pathsep + os.environ['PATH']

import json
import pathlib
import numpy as np
import torch
import torchaudio
import sounddevice as sd
import yaml
from PyQt6.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, 
                             QHBoxLayout, QPushButton, QLabel, QFileDialog, 
                             QMessageBox, QComboBox, QDoubleSpinBox, QSpinBox)
from PyQt6.QtCore import Qt
import pyqtgraph as pg

# Add current directory to sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class CustomViewBox(pg.ViewBox):
    def __init__(self, parent_gui):
        pg.ViewBox.__init__(self)
        self.parent_gui = parent_gui

    def mouseDragEvent(self, ev):
        if ev.button() == Qt.MouseButton.MiddleButton:
            ev.accept()
            # Manual Pan
            # mapToView maps from Item coordinates (pixels) to Data coordinates
            tr = self.mapToView(ev.pos()) - self.mapToView(ev.lastPos())
            self.translateBy(-tr)
        elif ev.button() == Qt.MouseButton.LeftButton or ev.button() == Qt.MouseButton.RightButton:
            # Block default ViewBox actions (Zoom/Pan)
            ev.accept()
        else:
            super().mouseDragEvent(ev)

    def wheelEvent(self, ev, axis=None):
        modifiers = QApplication.keyboardModifiers()
        if modifiers == Qt.KeyboardModifier.ControlModifier:
            # Zoom X (axis 0)
            super().wheelEvent(ev, axis=0)
        elif modifiers == Qt.KeyboardModifier.AltModifier:
            # Zoom Y (axis 1)
            super().wheelEvent(ev, axis=1)
        else:
            super().wheelEvent(ev, axis=axis)
            
    def mouseClickEvent(self, ev):
        # Ignore Left/Right clicks so they propagate or don't trigger ViewBox menus
        if ev.button() == Qt.MouseButton.LeftButton or ev.button() == Qt.MouseButton.RightButton:
            ev.ignore()
        else:
            super().mouseClickEvent(ev)

class PianoRollAxis(pg.AxisItem):
    def __init__(self, orientation='left', **kwargs):
        super().__init__(orientation, **kwargs)

    def tickStrings(self, values, scale, spacing):
        strings = []
        note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
        for v in values:
            # Only label if close to integer
            if abs(v - round(v)) > 0.1:
                strings.append("")
                continue
            
            try:
                # v is MIDI note number
                note_idx = int(round(v))
                octave = note_idx // 12 - 1
                name = note_names[note_idx % 12]
                strings.append(f"{name}{octave}")
            except:
                strings.append("")
        return strings
    
    def tickValues(self, minVal, maxVal, size):
        # Force integer ticks
        min_idx = int(np.ceil(minVal))
        max_idx = int(np.floor(maxVal))
        values = list(range(min_idx, max_idx + 1))
        return [(1.0, values)]

class BPMAxis(pg.AxisItem):
    def __init__(self, parent_gui, orientation='bottom', **kwargs):
        super().__init__(orientation, **kwargs)
        self.parent_gui = parent_gui

    def tickStrings(self, values, scale, spacing):
        if not self.parent_gui.config or not self.parent_gui.sr:
            return []
            
        hop_size = self.parent_gui.config.get('hop_size', 512)
        sr = self.parent_gui.config.get('audio_sample_rate', 44100)
        bpm = self.parent_gui.bpm_spin.value()
        beats_per_bar = self.parent_gui.beats_spin.value()
        
        strings = []
        for v in values:
            # v is frame index
            time_sec = v * hop_size / sr
            total_beats = time_sec * (bpm / 60.0)
            bar = int(total_beats / beats_per_bar) + 1
            beat = int(total_beats % beats_per_bar) + 1
            strings.append(f"{bar}-{beat}")
        return strings

try:
    from training.nsf_HiFigan_task import nsf_HiFigan, dynamic_range_compression_torch
    from utils.config_utils import read_full_config
    from utils.wav2F0 import get_pitch
    from utils.wav2mel import PitchAdjustableMelSpectrogram
except ImportError as e:
    print(f"Error importing modules: {e}")
    print("Please ensure you are running this script from the root of the SingingVocoders project.")

class VocalShifterGUI(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("SingingVocoders - 音高修正工具")
        self.resize(1200, 800)
        
        # Data
        self.model = None
        self.config = None
        self.audio = None
        self.sr = None
        self.f0 = None # numpy array
        self.f0_original = None # numpy array
        self.mel = None # tensor
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.synthesized_audio = None
        self.mel_transform = None
        
        # Interaction State
        self.is_drawing = False
        self.last_mouse_pos = None
        
        self.init_ui()
        
    def init_ui(self):
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        layout = QVBoxLayout(central_widget)
        
        # Toolbar
        toolbar = QHBoxLayout()
        
        self.btn_load_model = QPushButton("加载模型文件夹")
        self.btn_load_model.clicked.connect(self.load_model_dialog)
        
        self.btn_load_audio = QPushButton("加载音频")
        self.btn_load_audio.clicked.connect(self.load_audio_dialog)
        
        self.btn_play_orig = QPushButton("播放原音")
        self.btn_play_orig.clicked.connect(self.play_original)
        
        self.btn_synthesize = QPushButton("合成并播放")
        self.btn_synthesize.clicked.connect(self.synthesize_and_play)
        
        self.btn_stop = QPushButton("停止")
        self.btn_stop.clicked.connect(self.stop_audio)

        # self.edit_mode = QComboBox()
        # self.edit_mode.addItems(["查看/拖拽", "绘制音高"])
        
        self.shift_spin = QDoubleSpinBox()
        self.shift_spin.setRange(-24, 24)
        self.shift_spin.setSingleStep(1)
        self.shift_spin.setPrefix("移调: ")
        self.shift_spin.setSuffix(" 半音")
        self.shift_spin.valueChanged.connect(self.apply_shift)
        
        self.bpm_spin = QDoubleSpinBox()
        self.bpm_spin.setRange(10, 300)
        self.bpm_spin.setValue(120)
        self.bpm_spin.setPrefix("BPM: ")
        self.bpm_spin.valueChanged.connect(lambda: self.plot_widget.getAxis('bottom').update())

        self.beats_spin = QSpinBox()
        self.beats_spin.setRange(1, 32)
        self.beats_spin.setValue(4)
        self.beats_spin.setPrefix("拍号: ")
        self.beats_spin.setSuffix(" / 4")
        self.beats_spin.valueChanged.connect(lambda: self.plot_widget.getAxis('bottom').update())

        toolbar.addWidget(self.btn_load_model)
        toolbar.addWidget(self.btn_load_audio)
        # toolbar.addWidget(self.edit_mode)
        toolbar.addWidget(self.shift_spin)
        toolbar.addWidget(self.bpm_spin)
        toolbar.addWidget(self.beats_spin)
        toolbar.addWidget(self.btn_play_orig)
        toolbar.addWidget(self.btn_synthesize)
        toolbar.addWidget(self.btn_stop)
        layout.addLayout(toolbar)
        
        # Plot Area
        self.plot_widget = pg.PlotWidget(
            viewBox=CustomViewBox(self), 
            axisItems={
                'left': PianoRollAxis(orientation='left'),
                'bottom': BPMAxis(self, orientation='bottom')
            }
        )
        self.plot_widget.setBackground('#2b2b2b') # Dark background for piano roll look
        self.plot_widget.setLabel('left', '音高 (Note)')
        self.plot_widget.setLabel('bottom', '小节-拍')
        self.plot_widget.showGrid(x=True, y=True, alpha=0.5)
        self.plot_widget.getAxis('left').setGrid(128) # Try to hint grid density
        # self.plot_widget.showAxis('top') # Removed top axis
        self.plot_widget.setMouseEnabled(x=True, y=True)
        
        # Waveform View (Fixed Y-axis)
        self.waveform_view = pg.ViewBox()
        self.waveform_view.setMouseEnabled(x=False, y=False)
        self.waveform_view.setMenuEnabled(False)
        # Critical: Disable mouse interaction so it doesn't steal events from the main plot
        self.waveform_view.setAcceptedMouseButtons(Qt.MouseButton.NoButton)
        self.waveform_view.setXLink(self.plot_widget.plotItem.vb)
        self.waveform_view.setYRange(-1, 1)
        self.waveform_view.setZValue(-1) # Behind F0
        self.plot_widget.scene().addItem(self.waveform_view)
        
        # Connect resize event to update waveform view geometry
        self.plot_widget.plotItem.vb.sigResized.connect(self.update_views)

        # Custom Mouse Interaction via Scene Signals (More robust)
        self.plot_widget.scene().sigMouseMoved.connect(self.on_scene_mouse_move)
        self.plot_widget.scene().sigMouseClicked.connect(self.on_scene_mouse_click)
        
        # Curves
        # Waveform background
        self.waveform_curve = pg.PlotCurveItem(pen=pg.mkPen(color=(255, 255, 255, 30), width=1), name="Waveform")
        self.waveform_view.addItem(self.waveform_curve)
        
        # Original F0 (faint)
        self.f0_orig_curve_item = self.plot_widget.plot(pen=pg.mkPen(color=(255, 255, 255, 80), width=2, style=Qt.PenStyle.DashLine), name="Original F0")

        # We use a scatter plot for points or a simple plot. Plot is faster.
        self.f0_curve_item = self.plot_widget.plot(pen=pg.mkPen('#00ff00', width=3), name="F0")
        
        layout.addWidget(self.plot_widget)
        
        # Instructions
        instructions = QLabel("使用说明: 加载模型 -> 加载音频 -> 左键绘制音高，右键还原音高，中键拖动，Ctrl+滚轮缩放X，Alt+滚轮缩放Y -> 合成")
        layout.addWidget(instructions)
        
        # Status
        self.status_label = QLabel("就绪")
        layout.addWidget(self.status_label)

    def update_views(self):
        self.waveform_view.setGeometry(self.plot_widget.plotItem.vb.sceneBoundingRect())
        self.waveform_view.linkedViewChanged(self.plot_widget.plotItem.vb, self.waveform_view.XAxis)

    def load_model_dialog(self):
        folder = QFileDialog.getExistingDirectory(self, "选择模型文件夹")
        if folder:
            self.load_model(folder)

    def load_model(self, folder):
        try:
            folder_path = pathlib.Path(folder)
            # Check for config
            config_path = folder_path / 'config.yaml'
            if not config_path.exists():
                config_path = folder_path / 'config.json'
            
            if not config_path.exists():
                raise FileNotFoundError("目录中未找到 config.yaml 或 config.json。")
            
            # Load config
            if config_path.suffix == '.yaml':
                self.config = read_full_config(config_path)
            else:
                with open(config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            
            # Patch config for missing keys
            if 'clip_grad_norm' not in self.config:
                self.config['clip_grad_norm'] = 1.0
            
            # Map keys for compatibility
            key_mapping = {
                'sampling_rate': 'audio_sample_rate',
                'num_mels': 'audio_num_mel_bins',
                'n_fft': 'fft_size'
            }
            for old_key, new_key in key_mapping.items():
                if old_key in self.config and new_key not in self.config:
                    self.config[new_key] = self.config[old_key]

            # Add default F0 extraction parameters if missing
            if 'f0_min' not in self.config:
                self.config['f0_min'] = 40
            if 'f0_max' not in self.config:
                self.config['f0_max'] = 1600

            # Restructure config for model_args if needed
            if 'model_args' not in self.config:
                model_arg_keys = [
                    'mini_nsf', 'upsample_rates', 'upsample_kernel_sizes', 
                    'upsample_initial_channel', 'resblock_kernel_sizes', 
                    'resblock_dilation_sizes', 'resblock', 'discriminator_periods'
                ]
                self.config['model_args'] = {}
                for key in model_arg_keys:
                    if key in self.config:
                        self.config['model_args'][key] = self.config[key]

            # Check for checkpoint
            ckpt_path = folder_path / 'model.ckpt'
            if not ckpt_path.exists():
                # Try to find any ckpt
                ckpts = list(folder_path.glob('*.ckpt'))
                if ckpts:
                    ckpt_path = ckpts[0]
                else:
                    raise FileNotFoundError("目录中未找到 .ckpt 文件。")
            
            self.status_label.setText(f"正在加载模型 {ckpt_path}...")
            QApplication.processEvents()
            
            # Initialize Model
            self.model = nsf_HiFigan(self.config)
            self.model.build_model()
            
            # Load weights
            checkpoint = torch.load(ckpt_path, map_location='cpu')
            state_dict = checkpoint.get('state_dict', checkpoint)
            
            # Handle nested generator checkpoint (common in some releases)
            if 'generator' in state_dict and isinstance(state_dict['generator'], dict) and len(state_dict) == 1:
                print("Detected nested generator checkpoint. Loading directly into generator...")
                self.model.generator.load_state_dict(state_dict['generator'])
            else:
                # Try loading with strict=False to allow missing discriminator weights
                # This is common for inference-only checkpoints
                missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
                
                if missing_keys:
                    print(f"Warning: Missing keys in state_dict (likely discriminator): {len(missing_keys)} keys")
                    # Verify if generator keys are present
                    gen_missing = [k for k in missing_keys if k.startswith('generator')]
                    if gen_missing:
                        print(f"Critical Warning: Generator keys are missing! {gen_missing[:5]}...")
                        QMessageBox.warning(self, "警告", f"模型权重可能不完整！\n检测到生成器参数缺失 (例如 {gen_missing[0]})\n合成结果可能是噪音。")
                
                if unexpected_keys:
                    print(f"Warning: Unexpected keys in state_dict: {len(unexpected_keys)} keys")

            self.model.to(self.device)
            self.model.eval()
            
            # Initialize Mel Transform
            self.mel_transform = PitchAdjustableMelSpectrogram(
                sample_rate=self.config['audio_sample_rate'],
                n_fft=self.config['fft_size'],
                win_length=self.config['win_size'],
                hop_length=self.config['hop_size'],
                f_min=self.config['fmin'],
                f_max=self.config['fmax'],
                n_mels=self.config['audio_num_mel_bins']
            )
            
            self.status_label.setText(f"模型已加载: {folder_path.name}")
            
        except Exception as e:
            QMessageBox.critical(self, "错误", f"加载模型失败: {str(e)}")
            self.status_label.setText("模型加载失败。")

    def load_audio_dialog(self):
        file_path, _ = QFileDialog.getOpenFileName(self, "选择音频文件", "", "音频文件 (*.wav *.flac *.mp3)")
        if file_path:
            self.load_audio(file_path)

    def load_audio(self, file_path):
        if self.model is None:
            QMessageBox.warning(self, "警告", "请先加载模型以确保采样率正确。")
            return
            
        try:
            self.status_label.setText(f"正在加载音频 {file_path}...")
            QApplication.processEvents()
            
            audio, sr = torchaudio.load(file_path)
            # Mix to mono if needed
            if audio.shape[0] > 1:
                audio = torch.mean(audio, dim=0, keepdim=True)
            
            # Resample
            target_sr = self.config['audio_sample_rate']
            if sr != target_sr:
                resampler = torchaudio.transforms.Resample(sr, target_sr)
                audio = resampler(audio)
                sr = target_sr
            
            self.audio = audio
            self.sr = sr
            
            # Extract Mel and F0
            self.status_label.setText("正在提取 Mel 谱和 F0...")
            QApplication.processEvents()
            
            # Mel
            # key_shift=0 for original
            self.mel = dynamic_range_compression_torch(self.mel_transform(self.audio, key_shift=0))
            
            # F0
            # Use 'parselmouth' as default pitch extractor
            f0_np, uv = get_pitch(
                'parselmouth',
                self.audio[0].numpy(), 
                hparams=self.config, 
                speed=1, 
                interp_uv=True, 
                length=self.mel.shape[2] # Mel time dimension
            )
            
            # Convert F0 to MIDI
            self.f0 = np.zeros_like(f0_np)
            # Avoid log(0)
            mask = f0_np > 0
            self.f0[mask] = 69 + 12 * np.log2(f0_np[mask] / 440.0)
            self.f0[~mask] = np.nan # Use NaN for unvoiced to break the line
            
            self.f0_original = self.f0.copy()
            
            # Prepare waveform for display
            # Downsample for performance
            audio_np = self.audio[0].numpy()
            hop_size = self.config['hop_size']
            # We want to plot waveform against frames.
            # x = sample_idx / hop_size
            # Let's downsample to e.g. 1 point per 0.1 frame or so, or just enough to look good.
            # Actually, if we have too many points, pyqtgraph might be slow.
            # Let's aim for ~1000-2000 points per visible area? 
            # For now, just downsample by a fixed factor, e.g. hop_size/4
            ds_factor = max(1, int(hop_size / 4))
            audio_ds = audio_np[::ds_factor]
            x_ds = np.arange(len(audio_ds)) * ds_factor / hop_size
            
            # Center waveform at MIDI 60 (C4), scale amplitude to +/- 24 semitones
            # self.waveform_y = 60 + audio_ds * 24
            # Now using separate viewbox, so just use raw amplitude
            self.waveform_y = audio_ds
            self.waveform_x = x_ds

            # Plot
            self.update_plot()
            self.plot_widget.autoRange() # Only auto-range when loading new audio
            self.update_views() # Ensure waveform view is synced
            self.status_label.setText("音频已加载。")
            
        except Exception as e:
            QMessageBox.critical(self, "错误", f"加载音频失败: {str(e)}")
            self.status_label.setText("音频加载失败。")

    def update_plot(self):
        if hasattr(self, 'waveform_x') and self.waveform_x is not None:
            self.waveform_curve.setData(self.waveform_x, self.waveform_y)

        if self.f0_original is not None:
            self.f0_orig_curve_item.setData(self.f0_original, connect="finite")

        if self.f0 is not None:
            # Connect points only if they are not NaN
            self.f0_curve_item.setData(self.f0, connect="finite")
            # self.plot_widget.autoRange() # Removed to prevent view reset on every update

    def on_scene_mouse_move(self, pos):
        if self.f0 is None:
            return
            
        # Check global mouse buttons
        buttons = QApplication.mouseButtons()
        
        is_left = bool(buttons & Qt.MouseButton.LeftButton)
        is_right = bool(buttons & Qt.MouseButton.RightButton)
        
        if not (is_left or is_right):
            self.last_mouse_pos = None # Reset if no buttons pressed
            return

        # Map scene pos to view coordinates
        # Note: pos passed to sigMouseMoved is already in scene coordinates
        mouse_point = self.plot_widget.plotItem.vb.mapSceneToView(pos)
        
        self.handle_draw(mouse_point, is_left, is_right)

    def on_scene_mouse_click(self, ev):
        if self.f0 is None:
            return
            
        if ev.button() == Qt.MouseButton.LeftButton or ev.button() == Qt.MouseButton.RightButton:
            ev.accept()
            mouse_point = self.plot_widget.plotItem.vb.mapSceneToView(ev.scenePos())
            
            is_left = (ev.button() == Qt.MouseButton.LeftButton)
            is_right = (ev.button() == Qt.MouseButton.RightButton)
            
            # Reset last pos for single click
            self.last_mouse_pos = None
            self.handle_draw(mouse_point, is_left, is_right)

    def handle_draw(self, point, is_left, is_right):
        x = int(point.x())
        y = point.y()
        
        if 0 <= x < len(self.f0):
            # Interpolation logic
            if self.last_mouse_pos is not None:
                last_x, last_y = self.last_mouse_pos
                # Ensure we draw from left to right or right to left covering all frames
                start_x, end_x = sorted((last_x, x))
                start_x = max(0, start_x)
                end_x = min(len(self.f0) - 1, end_x)
                
                if start_x < end_x:
                    # Linear interpolation
                    for i in range(start_x, end_x + 1):
                        if is_left:
                            # Draw new pitch
                            # Calculate interpolated y
                            ratio = (i - last_x) / (x - last_x) if x != last_x else 0
                            interp_y = last_y + ratio * (y - last_y)
                            self.f0[i] = interp_y
                        elif is_right:
                            # Restore original pitch
                            if self.f0_original is not None:
                                self.f0[i] = self.f0_original[i]
                else:
                    # Single point (vertical move or same x)
                    if is_left:
                        self.f0[x] = y
                    elif is_right and self.f0_original is not None:
                        self.f0[x] = self.f0_original[x]
            else:
                # First point of drag or click
                if is_left:
                    self.f0[x] = y
                elif is_right and self.f0_original is not None:
                    self.f0[x] = self.f0_original[x]
            
            self.last_mouse_pos = (x, y)
            self.update_plot()

    # Removed old custom_mouse_drag, mouse_moved, mouse_clicked methods
    # to avoid confusion
    
    def mouseReleaseEvent(self, ev):
        self.last_mouse_pos = None
        super().mouseReleaseEvent(ev)

    def apply_shift(self, semitones):
        if self.f0_original is not None:
            # Shift original F0 (MIDI)
            self.f0 = self.f0_original + semitones
            self.update_plot()

    def play_original(self):
        if self.audio is not None:
            sd.stop()
            # self.audio is (C, N), sd.play expects (N, C) or (N,)
            audio_np = self.audio.numpy().T
            sd.play(audio_np, self.sr)

    def synthesize_and_play(self):
        if self.model is None or self.audio is None:
            return
        
        try:
            self.status_label.setText("正在合成...")
            QApplication.processEvents()
            
            # Prepare inputs
            mel_tensor = self.mel.to(self.device)
            
            # Convert MIDI back to Hz
            f0_hz = np.zeros_like(self.f0)
            # Handle NaNs (unvoiced) -> 0 Hz
            mask = ~np.isnan(self.f0)
            f0_hz[mask] = 440.0 * (2 ** ((self.f0[mask] - 69) / 12.0))
            f0_hz[~mask] = 0
            
            f0_tensor = torch.from_numpy(f0_hz).float().unsqueeze(0).to(self.device)
            
            # Debug: Check if F0 is actually different
            # print(f"F0 Mean: {f0_tensor.mean().item()}")
            
            with torch.no_grad():
                output = self.model.Gforward(sample={'mel': mel_tensor, 'f0': f0_tensor})['audio']
            
            self.synthesized_audio = output[0].cpu().numpy()
            
            # Fix shape for sounddevice: (C, N) -> (N,) or (N, C)
            if self.synthesized_audio.ndim == 2 and self.synthesized_audio.shape[0] == 1:
                 self.synthesized_audio = self.synthesized_audio.squeeze(0)
            
            # Update waveform display with synthesized audio
            audio_np = self.synthesized_audio
            hop_size = self.config['hop_size']
            ds_factor = max(1, int(hop_size / 4))
            audio_ds = audio_np[::ds_factor]
            x_ds = np.arange(len(audio_ds)) * ds_factor / hop_size
            
            self.waveform_y = audio_ds
            self.waveform_x = x_ds
            self.update_plot()
            
            self.status_label.setText("正在播放合成音频...")
            sd.stop()
            sd.play(self.synthesized_audio, self.sr)
            self.status_label.setText("完成。")
            
        except Exception as e:
            QMessageBox.critical(self, "错误", f"合成失败: {str(e)}")
            self.status_label.setText("合成失败。")

    def stop_audio(self):
        sd.stop()

if __name__ == '__main__':
    app = QApplication(sys.argv)
    window = VocalShifterGUI()
    window.show()
    sys.exit(app.exec())
